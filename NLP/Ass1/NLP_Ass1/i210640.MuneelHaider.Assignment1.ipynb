{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muneel Haider\n",
    "\n",
    "i21-0640\n",
    "\n",
    "Assignment 1 - NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Muneel\n",
      "[nltk_data]     Haider\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict, Counter\n",
    "import string\n",
    "import random\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCSV(file):\n",
    "    print(\"Loading csv file.\")\n",
    "    return pd.read_csv(file)\n",
    "\n",
    "def cleanAndTokenize(textRead):\n",
    "    textRead = textRead.lower()\n",
    "    textRead = textRead.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = word_tokenize(textRead)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildNGramModel(tokensList, n=1):\n",
    "    print(f\"Building {n}-gram model...\")\n",
    "    model = defaultdict(Counter)\n",
    "    for tokens in tokensList:\n",
    "        for i in range(len(tokens) - (n - 1)):\n",
    "            modelDictionary = tuple(tokens[i:i + n - 1])\n",
    "            model[modelDictionary][tokens[i + n - 1]] += 1\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictNextWord(model, modelDictionary):\n",
    "    modelDictionary = tuple(modelDictionary)\n",
    "    if modelDictionary in model:\n",
    "        return max(model[modelDictionary], key=model[modelDictionary].get)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def predictFrequentWord(model):\n",
    "    return max(model[()], key=model[()].get)\n",
    "\n",
    "def generateSentence(model, modelDictionary, countWords=10):\n",
    "    sentence = list(modelDictionary)\n",
    "    for _ in range(countWords - len(modelDictionary)):\n",
    "        nextWord = predictNextWord(model, modelDictionary)\n",
    "        if not nextWord:\n",
    "            break\n",
    "        sentence.append(nextWord)\n",
    "        modelDictionary = tuple(sentence[-len(modelDictionary):])\n",
    "    return ' '.join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reviewClassifier(reviewTokens, model):\n",
    "    print(\"Classifying review...\")\n",
    "    positiveScore = sum(model[()][token] for token in reviewTokens if token in model[()])\n",
    "    negativeScore = sum(-model[()][token] for token in reviewTokens if token not in model[()])\n",
    "    return \"Positive\" if positiveScore > negativeScore else \"Negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading csv file.\n",
      "Building 1-gram model...\n",
      "Building 2-gram model...\n",
      "Building 3-gram model...\n",
      "Generated Unigram Sentence: the\n",
      "\n",
      "Generated Bigram Sentence: the film is a lot of the film is a\n",
      "\n",
      "Generated Trigram Sentence: the film is a very good and the film is\n",
      "\n",
      "Classifying review...\n",
      "Unigram Sentence Classification: Positive\n",
      "\n",
      "Classifying review...\n",
      "Bigram Sentence Classification: Positive\n",
      "\n",
      "Classifying review...\n",
      "Trigram Sentence Classification: Positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = 'IMDB Dataset.csv'\n",
    "data = readCSV(file)\n",
    "data['tokens'] = data['review'].apply(cleanAndTokenize)\n",
    "\n",
    "Unigram = buildNGramModel(data['tokens'], 1)\n",
    "Bigram = buildNGramModel(data['tokens'], 2)\n",
    "Trigram = buildNGramModel(data['tokens'], 3)\n",
    "\n",
    "unigramSentence = generateSentence(Unigram, (predictFrequentWord(Unigram),), 10)\n",
    "bigramSentence = generateSentence(Bigram, (unigramSentence.split()[0],), 10)\n",
    "trigramSentence = generateSentence(Trigram, tuple(bigramSentence.split()[:2]), 10)\n",
    "\n",
    "print(\"Generated Unigram Sentence:\", unigramSentence)\n",
    "print()\n",
    "print(\"Generated Bigram Sentence:\", bigramSentence)\n",
    "print()\n",
    "print(\"Generated Trigram Sentence:\", trigramSentence)\n",
    "print()\n",
    "\n",
    "print(\"Unigram Sentence Classification:\", reviewClassifier(unigramSentence.split(), Unigram))\n",
    "print()\n",
    "print(\"Bigram Sentence Classification:\", reviewClassifier(bigramSentence.split(), Unigram))\n",
    "print()\n",
    "print(\"Trigram Sentence Classification:\", reviewClassifier(trigramSentence.split(), Unigram))\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
