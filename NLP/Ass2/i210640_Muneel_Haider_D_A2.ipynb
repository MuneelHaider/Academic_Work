{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muneel Haider\n",
    "i21-0640\n",
    "\n",
    "NLP - Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(r'factuality_annotations_xsum_summaries.csv')\n",
    "\n",
    "dataset = dataset.dropna()  # Remove missing values\n",
    "\n",
    "dataset['is_factual'] = dataset['is_factual'].map(lambda x: 1 if x == 'yes' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    return text.lower()  # Convert to lowercase\n",
    "\n",
    "dataset['summary'] = dataset['summary'].apply(clean_text)\n",
    "\n",
    "# Calculating term frequency\n",
    "def term_frequency(text):\n",
    "    words = text.split()\n",
    "    total_words = len(words)\n",
    "    tf_dict = Counter(words)\n",
    "    \n",
    "    for word in tf_dict:\n",
    "        tf_dict[word] /= total_words\n",
    "    return tf_dict\n",
    "\n",
    "# Calculate inverse document frequency\n",
    "def inverse_document_frequency(corpus):\n",
    "    doc_count = len(corpus)\n",
    "    idf_dict = {}\n",
    "    \n",
    "    for doc in corpus:\n",
    "        for word in set(doc.split()):\n",
    "            idf_dict[word] = idf_dict.get(word, 0) + 1\n",
    "    \n",
    "    for word in idf_dict:\n",
    "        idf_dict[word] = np.log(doc_count / idf_dict[word])\n",
    "    \n",
    "    return idf_dict\n",
    "\n",
    "# Calculate term_frequency and inverse_document_frequency of each document\n",
    "def compute_tf_idf(tf, idf):\n",
    "    tf_idf_dict = {}\n",
    "    for word in tf:\n",
    "        tf_idf_dict[word] = tf[word] * idf.get(word, 0)\n",
    "    return tf_idf_dict\n",
    "\n",
    "# inverse_document_frequency of entire Corpus\n",
    "corpus = dataset['summary'].tolist()\n",
    "idf_values = inverse_document_frequency(corpus)\n",
    "\n",
    "# text to vector representation\n",
    "def text_to_vector(text, vocab_dict, idf_values):\n",
    "    tf = term_frequency(text)\n",
    "    tf_idf = compute_tf_idf(tf, idf_values)\n",
    "    vector = np.zeros(len(vocab_dict))\n",
    "    \n",
    "    for word, idx in vocab_dict.items():\n",
    "        vector[idx] = tf_idf.get(word, 0)\n",
    "    \n",
    "    return vector\n",
    "\n",
    "vocabulary = set()\n",
    "for summary in dataset['summary']:\n",
    "    vocabulary.update(summary.split())\n",
    "\n",
    "vocab_dict = {word: i for i, word in enumerate(vocabulary)}\n",
    "\n",
    "X = np.array([text_to_vector(summary, vocab_dict, idf_values) for summary in dataset['summary']])\n",
    "y = dataset['is_factual'].values\n",
    "\n",
    "def split_data(X, y, test_size=0.2, random_seed=42):\n",
    "    np.random.seed(random_seed)\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    split_index = int(X.shape[0] * (1 - test_size))\n",
    "    \n",
    "    train_indices = indices[:split_index]\n",
    "    test_indices = indices[split_index:]\n",
    "    \n",
    "    return X[train_indices], X[test_indices], y[train_indices], y[test_indices]\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_data(X, y)\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, epochs=1000, class_weights=None):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.weights = None\n",
    "        self.bias = 0\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.weights = np.zeros(num_features)\n",
    "        \n",
    "        for _ in range(self.epochs):\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_pred = self.sigmoid(linear_model)\n",
    "            \n",
    "            dw = np.zeros(num_features)\n",
    "            db = 0\n",
    "            \n",
    "            for i in range(num_samples):\n",
    "                error = y_pred[i] - y[i]\n",
    "                weight = self.class_weights.get(y[i], 1) if self.class_weights else 1\n",
    "                \n",
    "                dw += weight * error * X[i]\n",
    "                db += weight * error\n",
    "            \n",
    "            dw /= num_samples\n",
    "            db /= num_samples\n",
    "\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_prob = self.sigmoid(linear_model)\n",
    "        return np.array([1 if prob > 0.5 else 0 for prob in y_prob])\n",
    "\n",
    "class_counts = np.bincount(y_train)\n",
    "class_weights = {0: 1.0, 1: class_counts[0] / class_counts[1]}\n",
    "\n",
    "model = LogisticRegression(learning_rate=0.01, epochs=1000, class_weights=class_weights)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "def evaluate_metrics(y_true, y_pred):\n",
    "    true_pos = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    true_neg = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    false_pos = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    false_neg = np.sum((y_true == 1) & (y_pred == 0))\n",
    "\n",
    "    accuracy = (true_pos + true_neg) / len(y_true)\n",
    "    \n",
    "    precision = true_pos / (true_pos + false_pos) if (true_pos + false_pos) != 0 else 0\n",
    "    recall = true_pos / (true_pos + false_neg) if (true_pos + false_neg) != 0 else 0\n",
    "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "    return accuracy, precision, recall, f1_score\n",
    "\n",
    "accuracy, precision, recall, f1 = evaluate_metrics(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1 Score: {f1:.3f}\")\n",
    "\n",
    "def confusion_matrix(y_true, y_pred):\n",
    "    true_pos = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    true_neg = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    false_pos = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    false_neg = np.sum((y_true == 1) & (y_pred == 0))\n",
    "\n",
    "    matrix = np.array([[true_neg, false_pos], [false_neg, true_pos]])\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(f\"          Predicted: 0    Predicted: 1\")\n",
    "    print(f\"Actual: 0    {matrix[0][0]}            {matrix[0][1]}\")\n",
    "    print(f\"Actual: 1    {matrix[1][0]}             {matrix[1][1]}\")\n",
    "\n",
    "confusion_matrix(y_test, y_pred)\n",
    "\n",
    "def k_fold_cv(X, y, k=5):\n",
    "    fold_size = len(X) // k\n",
    "    accuracy_list = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        start = i * fold_size\n",
    "        end = start + fold_size\n",
    "        \n",
    "        X_val, y_val = X[start:end], y[start:end]\n",
    "        X_train_fold = np.concatenate([X[:start], X[end:]], axis=0)\n",
    "        y_train_fold = np.concatenate([y[:start], y[end:]], axis=0)\n",
    "        \n",
    "        model_cv = LogisticRegression(learning_rate=0.01, epochs=1000, class_weights=class_weights)\n",
    "        model_cv.fit(X_train_fold, y_train_fold)\n",
    "        y_pred_fold = model_cv.predict(X_val)\n",
    "        \n",
    "        fold_accuracy, _, _, _ = evaluate_metrics(y_val, y_pred_fold)\n",
    "        accuracy_list.append(fold_accuracy)\n",
    "    \n",
    "    return np.mean(accuracy_list), np.std(accuracy_list)\n",
    "\n",
    "average_accuracy, accuracy_std = k_fold_cv(X, y)\n",
    "\n",
    "print(f\"Average Accuracy using K-Fold cross-validation: {average_accuracy:.3f}\")\n",
    "print(f\"Standard Deviation of Accuracy: {accuracy_std:.3f}\")\n",
    "\n",
    "misclassified_idx = np.where(y_pred != y_test)[0]\n",
    "misclassified_examples = dataset.iloc[misclassified_idx]\n",
    "\n",
    "true_labels = y_test[misclassified_idx]\n",
    "pred_labels = y_pred[misclassified_idx]\n",
    "\n",
    "print(f\"\\nMisclassified examples: {len(misclassified_examples)}\")\n",
    "\n",
    "print(\"\\nMisclassified examples:\")\n",
    "for i in range(min(5, len(misclassified_examples))):\n",
    "    print(f\"Summary: {misclassified_examples.iloc[i]['summary']}\")\n",
    "    print(f\"Actual Label: {true_labels[i]}, Predicted Label: {pred_labels[i]}\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
