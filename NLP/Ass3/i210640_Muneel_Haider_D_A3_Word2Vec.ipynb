{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.utils import resample\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing completed.\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"spam_or_not_spam.csv\") \n",
    "\n",
    "data = data.dropna(subset=['email'])\n",
    "data['email'] = data['email'].astype(str)\n",
    "\n",
    "spamData = data[data['label'] == 1]\n",
    "notSpamData = data[data['label'] == 0]\n",
    "\n",
    "# Upsample the minority class (spam emails)\n",
    "unsampledSpam = resample(spamData, \n",
    "                          replace=True, \n",
    "                          n_samples=len(notSpamData), \n",
    "                          random_state=42)\n",
    "\n",
    "# Combine the balanced data\n",
    "newData = pd.concat([unsampledSpam, notSpamData]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Function to preprocess text\n",
    "def preProcessData(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the input text by removing numbers, special characters, and converting to lowercase.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation and special characters\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing and tokenize\n",
    "newData['cleaned_email'] = newData['email'].apply(preProcessData)\n",
    "newData['tokens'] = newData['cleaned_email'].apply(lambda x: x.split())\n",
    "\n",
    "print(\"Data preprocessing completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Vocabulary and Initialize Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 34212\n"
     ]
    }
   ],
   "source": [
    "# Build the vocabulary\n",
    "corpus = newData['tokens'].tolist()\n",
    "wordCount = Counter([word for sentence in corpus for word in sentence])\n",
    "vocab = list(wordCount.keys())\n",
    "wordToIndex = {word: i for i, word in enumerate(vocab)}\n",
    "indexToWord = {i: word for i, word in enumerate(vocab)}\n",
    "vocabSize = len(vocab)\n",
    "\n",
    "print(f\"Vocabulary size: {vocabSize}\")\n",
    "\n",
    "# Initialize word and context embeddings\n",
    "embeddingsDIM = 10\n",
    "wordEmbeddings = np.random.uniform(-1, 1, (vocabSize, embeddingsDIM))\n",
    "contextEmbeddings = np.random.uniform(-1, 1, (vocabSize, embeddingsDIM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def generateTrainingData(corpus, windowSize):\n",
    "    pairs = []\n",
    "    for sentence in corpus:\n",
    "        for centerIndex, centerWord in enumerate(sentence):\n",
    "            startWindow = max(0, centerIndex - windowSize)\n",
    "            endWindow = min(len(sentence), centerIndex + windowSize + 1)\n",
    "            for contextIndex in range(startWindow, endWindow):\n",
    "                if contextIndex != centerIndex and contextIndex < len(sentence):\n",
    "                    contextWord = sentence[contextIndex]\n",
    "                    pairs.append((centerWord, contextWord))\n",
    "    return pairs\n",
    "\n",
    "def getNegativeSamples(targetIndex, vocabSize, numberOfSamples):\n",
    "    samples = []\n",
    "    while len(samples) < numberOfSamples:\n",
    "        sampleIndex = random.randint(0, vocabSize - 1)\n",
    "        if sampleIndex != targetIndex:\n",
    "            samples.append(sampleIndex)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Training Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training pairs: 5224154\n"
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "windowSize = 2 \n",
    "negativeSamples = 5 \n",
    "\n",
    "trainingPairs = generateTrainingData(corpus, windowSize)\n",
    "print(f\"Total training pairs: {len(trainingPairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "learningRate = 0.01\n",
    "epochs = 5\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    lotalLoss = 0\n",
    "    random.shuffle(trainingPairs)  # Shuffle training data each epoch\n",
    "    \n",
    "    for centerWord, contextWord in trainingPairs:\n",
    "        centerIndex = wordToIndex[centerWord]\n",
    "        contextIndex = wordToIndex[contextWord]\n",
    "        \n",
    "        # Positive sample\n",
    "        z = np.dot(wordEmbeddings[centerIndex], contextEmbeddings[contextIndex])\n",
    "        p_pos = sigmoid(z)\n",
    "        loss = -np.log(p_pos)\n",
    "        \n",
    "        # Gradients for positive sample\n",
    "        gradientCenter = (p_pos - 1) * contextEmbeddings[contextIndex]\n",
    "        gradientContext = (p_pos - 1) * wordEmbeddings[centerIndex]\n",
    "        \n",
    "        # Update embeddings for positive sample\n",
    "        wordEmbeddings[centerIndex] -= learningRate * gradientCenter\n",
    "        contextEmbeddings[contextIndex] -= learningRate * gradientContext\n",
    "        \n",
    "        # Negative sampling\n",
    "        negativeSampleIndices = getNegativeSamples(centerIndex, vocabSize, negativeSamples)\n",
    "        for negativeIndex in negativeSampleIndices:\n",
    "            z_neg = np.dot(wordEmbeddings[centerIndex], contextEmbeddings[negativeIndex])\n",
    "            p_neg = sigmoid(z_neg)\n",
    "            loss += -np.log(1 - p_neg)\n",
    "            \n",
    "            # Gradients for negative sample\n",
    "            gradientCenter_Negative = p_neg * contextEmbeddings[negativeIndex]\n",
    "            gradientContext_Negative = p_neg * wordEmbeddings[centerIndex]\n",
    "            \n",
    "            # Update embeddings for negative sample\n",
    "            wordEmbeddings[centerIndex] -= learningRate * gradientCenter_Negative\n",
    "            contextEmbeddings[negativeIndex] -= learningRate * gradientContext_Negative\n",
    "        \n",
    "        lotalLoss += loss\n",
    "    \n",
    "    averageLoss = lotalLoss / len(trainingPairs)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Average Loss: {averageLoss:.4f}\")\n",
    "\n",
    "print(\"Word2Vec training completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the Trained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained embeddings saved as 'wordEmbeddings.npy'\n"
     ]
    }
   ],
   "source": [
    "# Save the word embeddings\n",
    "np.save(\"wordEmbeddings.npy\", wordEmbeddings)\n",
    "print(\"Trained embeddings saved as 'wordEmbeddings.npy'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
