{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Training Loss: 7.994 | Validation Loss: 7.655\n",
      "\n",
      "\n",
      "Epoch: 2 | Training Loss: 7.273 | Validation Loss: 7.780\n",
      "\n",
      "\n",
      "Epoch: 3 | Training Loss: 7.108 | Validation Loss: 7.944\n",
      "\n",
      "\n",
      "Epoch: 4 | Training Loss: 7.010 | Validation Loss: 8.001\n",
      "\n",
      "\n",
      "Epoch: 5 | Training Loss: 6.928 | Validation Loss: 8.090\n",
      "\n",
      "\n",
      "Epoch: 6 | Training Loss: 6.836 | Validation Loss: 8.144\n",
      "\n",
      "\n",
      "Epoch: 7 | Training Loss: 6.740 | Validation Loss: 8.233\n",
      "\n",
      "\n",
      "Epoch: 8 | Training Loss: 6.636 | Validation Loss: 8.320\n",
      "\n",
      "\n",
      "Epoch: 9 | Training Loss: 6.520 | Validation Loss: 8.381\n",
      "\n",
      "\n",
      "Epoch: 10 | Training Loss: 6.406 | Validation Loss: 8.467\n",
      "\n",
      "\n",
      "Epoch: 11 | Training Loss: 6.249 | Validation Loss: 8.531\n",
      "\n",
      "\n",
      "Epoch: 12 | Training Loss: 6.094 | Validation Loss: 8.624\n",
      "\n",
      "\n",
      "Epoch: 13 | Training Loss: 5.966 | Validation Loss: 8.787\n",
      "\n",
      "\n",
      "Epoch: 14 | Training Loss: 5.838 | Validation Loss: 8.836\n",
      "\n",
      "\n",
      "Epoch: 15 | Training Loss: 5.661 | Validation Loss: 8.924\n",
      "\n",
      "\n",
      "Epoch: 16 | Training Loss: 5.459 | Validation Loss: 9.066\n",
      "\n",
      "\n",
      "Epoch: 17 | Training Loss: 5.220 | Validation Loss: 9.162\n",
      "\n",
      "\n",
      "Epoch: 18 | Training Loss: 5.016 | Validation Loss: 9.252\n",
      "\n",
      "\n",
      "Epoch: 19 | Training Loss: 4.801 | Validation Loss: 9.408\n",
      "\n",
      "\n",
      "Epoch: 20 | Training Loss: 4.627 | Validation Loss: 9.494\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Name:     Muneel Haider , Muhammad Abdullah\n",
    "# Roll No.:   21I-0640    ,     21I-0643\n",
    "# Section:                D\n",
    "# NLP-Project\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# file paths for the provided datasets. \n",
    "urduArabDev = r\"D:\\Work\\Academic Work\\Academic_Work\\NLP\\Project\\urd_Arab.dev\"\n",
    "engDev = r\"D:\\Work\\Academic Work\\Academic_Work\\NLP\\Project\\eng_Latn.dev\"\n",
    "urduArabDevTest = r\"D:\\Work\\Academic Work\\Academic_Work\\NLP\\Project\\urd_Arab.devtest\"\n",
    "engDevTest = r\"D:\\Work\\Academic Work\\Academic_Work\\NLP\\Project\\eng_Latn.devtest\"\n",
    "\n",
    "# Preprocessing data \n",
    "with open(urduArabDev, encoding='utf-8') as uDev, \\\n",
    "     open(engDev, encoding='utf-8') as eDev, \\\n",
    "     open(urduArabDevTest, encoding='utf-8') as uDevTest, \\\n",
    "     open(engDevTest, encoding='utf-8') as eDevTest:\n",
    "    \n",
    "    uSentence = uDev.readlines() + uDevTest.readlines() # Urdu sentences from data\n",
    "    eSentence = eDev.readlines() + eDevTest.readlines() # English sentence from data\n",
    "\n",
    "# Shuffling both english and urdu sentences in pairs. \n",
    "data = list(zip(uSentence, eSentence))\n",
    "random.shuffle(data)\n",
    "\n",
    "# Splitting the data according to the given constraints. \n",
    "trainData, tempData = train_test_split(data, test_size=0.3, random_state=42)\n",
    "valData, testData = train_test_split(tempData, test_size=0.5, random_state=42)\n",
    "\n",
    "# seperating data after splitting\n",
    "uTrain, eTrain = zip(*trainData) # seperating data for training (both urdu and english)\n",
    "uVal, eVal = zip(*valData) # for validation\n",
    "uTest, eTest = zip(*testData) # for testing\n",
    "\n",
    "# Vocabulary class for text data given in dataset\n",
    "class Vocabulary:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.wordIndex = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3} # text data for start, end and unknown words in dataset. \n",
    "        self.indexWord = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"} # reverse\n",
    "        self.wCount = 4 \n",
    "\n",
    "    def sentenceAdd(self, sentence):\n",
    "        \n",
    "        for word in sentence.split(): # splitting sentence to words\n",
    "            self.wordAdd(word) # adding word to vocabulary \n",
    "\n",
    "    def wordAdd(self, word):\n",
    "        \n",
    "        if word not in self.wordIndex: # if the word is not in vocabulary\n",
    "        \n",
    "            self.wordIndex[word] = self.wCount\n",
    "            self.indexWord[self.wCount] = word\n",
    "            self.wCount += 1\n",
    "\n",
    "    def encode(self, sentence):\n",
    "        return [self.wordIndex.get(word, self.wordIndex[\"<UNK>\"]) for word in sentence.split()]\n",
    "\n",
    "    def decode(self, indices):\n",
    "        return \" \".join([self.indexWord[idx] for idx in indices if idx not in (0, 1, 2)])\n",
    "\n",
    "# Initialize vocabularies\n",
    "uVocab = Vocabulary() # urdu Vocabulary \n",
    "eVocab = Vocabulary() # english Vocabulary\n",
    "\n",
    "# Adding sentences in their respective vocabulary\n",
    "for sentence in uTrain:\n",
    "    uVocab.sentenceAdd(sentence) \n",
    "\n",
    "for sentence in eTrain:\n",
    "    eVocab.sentenceAdd(sentence)\n",
    "\n",
    "# Class for translation of data. \n",
    "class dataTranslate(Dataset):\n",
    "    \n",
    "    # encoding source and target sentences. \n",
    "    def __init__(self, sSentence, tSentence, sVocab, tVOcab):\n",
    "    \n",
    "        self.sSentence = [sVocab.encode(s) for s in sSentence]\n",
    "        self.tSentence = [tVOcab.encode(s) for s in tSentence]\n",
    "\n",
    "    # sum of data data samples.\n",
    "    def __len__(self):\n",
    "        return len(self.sSentence)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "    \n",
    "        src = torch.tensor(self.sSentence[idx] + [uVocab.wordIndex[\"<EOS>\"]])\n",
    "        tgt = torch.tensor(self.tSentence[idx] + [eVocab.wordIndex[\"<EOS>\"]])\n",
    "        return src, tgt\n",
    "\n",
    "# Function for padding sequences. \n",
    "def collate(batch):\n",
    "    \n",
    "    sBatch, tBatch = zip(*batch) # seperating source and target sentences. \n",
    "    sBatch = pad_sequence(sBatch, padding_value=uVocab.wordIndex[\"<PAD>\"], batch_first=True)\n",
    "    tBatch = pad_sequence(tBatch, padding_value=eVocab.wordIndex[\"<PAD>\"], batch_first=True)\n",
    "    \n",
    "    return sBatch, tBatch\n",
    "\n",
    "# Datasets for preperation\n",
    "trainData = dataTranslate(uTrain, eTrain, uVocab, eVocab)\n",
    "valData = dataTranslate(uVal, eVal, uVocab, eVocab)\n",
    "testData = dataTranslate(uTest, eTest, uVocab, eVocab)\n",
    "\n",
    "\n",
    "# for training and evaluation\n",
    "trainLoad = DataLoader(trainData, batch_size=64, shuffle=True, collate_fn=collate) # training\n",
    "valLoad = DataLoader(valData, batch_size=64, shuffle=False, collate_fn=collate) # validating \n",
    "testLoad = DataLoader(testData, batch_size=64, shuffle=False, collate_fn=collate) # testing\n",
    "\n",
    "# Encode class for LSTM \n",
    "class encoder(nn.Module):\n",
    "    def __init__(self, iLayer, embLayer, hiddenLayer, dropout):\n",
    "        \n",
    "        super().__init__() # parent class\n",
    "        self.embedding = nn.Embedding(iLayer, embLayer)\n",
    "        self.rnn = nn.LSTM(embLayer, hiddenLayer, num_layers=2, bidirectional=True, dropout=dropout, batch_first=True)\n",
    "        self.fc_hidden = nn.Linear(hiddenLayer * 2, hiddenLayer)\n",
    "        self.fc_cell = nn.Linear(hiddenLayer * 2, hiddenLayer)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        \n",
    "        embed = self.dropout(self.embedding(src))\n",
    "        results, (hide, cell) = self.rnn(embed) # LSTM on embedded input. \n",
    "        hide = torch.tanh(self.fc_hidden(torch.cat((hide[-2], hide[-1]), dim=1))).unsqueeze(0)\n",
    "        cell = torch.tanh(self.fc_cell(torch.cat((cell[-2], cell[-1]), dim=1))).unsqueeze(0)\n",
    "        return results, hide, cell\n",
    "\n",
    "# Attention function for decoding\n",
    "class Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, encHiddenLayer, decHiddenLayer): \n",
    "        super().__init__() # parent class\n",
    "        self.attn = nn.Linear(encHiddenLayer * 2 + decHiddenLayer, decHiddenLayer)\n",
    "        self.v = nn.Parameter(torch.rand(decHiddenLayer))\n",
    "\n",
    "    def forward(self, encodeResult, hide):\n",
    "        hide = hide[-1].unsqueeze(1).repeat(1, encodeResult.size(1), 1)\n",
    "        energy = torch.tanh(self.attn(torch.cat((hide, encodeResult), dim=2)))\n",
    "        attention = torch.sum(self.v * energy, dim=2) # using dot product \n",
    "        return torch.softmax(attention, dim=1)\n",
    "\n",
    "# Decoder function using attention \n",
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, dimResult, dimEmbed, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(dimResult, dimEmbed) # targetting tokens from embedding layer. \n",
    "        self.rnn = nn.LSTM(dimEmbed + enc_hid_dim * 2, dec_hid_dim, num_layers=2, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(dec_hid_dim + enc_hid_dim * 2, dimResult)\n",
    "        self.attention = attention\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell, encOutput):\n",
    "        \n",
    "        if hidden.size(0) == 1:  # If only one layer, repeat it\n",
    "            hidden = hidden.repeat(2, 1, 1)\n",
    "            cell = cell.repeat(2, 1, 1)\n",
    "        \n",
    "        input = input.unsqueeze(1)\n",
    "        embedded = self.dropout(self.embedding(input)) \n",
    "        aWeights = self.attention(encOutput, hidden) # attention weights \n",
    "        context = torch.bmm(aWeights.unsqueeze(1), encOutput)\n",
    "        rnnInp = torch.cat((embedded, context), dim=2)\n",
    "        results, (hidden, cell) = self.rnn(rnnInp, (hidden, cell))\n",
    "        predictions = self.fc(torch.cat((results.squeeze(1), context.squeeze(1)), dim=1)) # generating predictions. \n",
    "        \n",
    "        return predictions, hidden, cell\n",
    "\n",
    "\n",
    "\n",
    "# Function for sequence to sequence learning\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, enc, dec, device):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.encoder = enc # encoder\n",
    "        self.decoder = dec # decoder\n",
    "        self.device = device # for running on cpu\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        targetLen = trg.shape[1]\n",
    "        batchSize = trg.shape[0]\n",
    "        results = torch.zeros(batchSize, targetLen, self.decoder.fc.out_features).to(self.device)\n",
    "        encOutput, hidden, cell = self.encoder(src)\n",
    "        input = trg[:, 0] # initialising decoding \n",
    "        \n",
    "        for t in range(1, targetLen):\n",
    "        \n",
    "            output, hidden, cell = self.decoder(input, hidden, cell, encOutput)\n",
    "            results[:, t, :] = output\n",
    "            input = output.argmax(1) # highest scording token as next input using argmax. \n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "# Initializing all the parameters for the model. \n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # for both cpu and gpu (one member had gpu and one had cpu).\n",
    "iVocabSize = len(uVocab.wordIndex)\n",
    "oVocabSize = len(eVocab.wordIndex)\n",
    "encEmbed = 256\n",
    "decEmbed = 256\n",
    "hiddenLayer = 512\n",
    "dropout = 0.3\n",
    "\n",
    "attention = Attention(hiddenLayer, hiddenLayer)\n",
    "encoder = encoder(iVocabSize, encEmbed, hiddenLayer, dropout)\n",
    "decoder = Decoder(oVocabSize, decEmbed, hiddenLayer, hiddenLayer, dropout, attention)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device) # encoder decoder for sequence model. \n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=eVocab.wordIndex[\"<PAD>\"]) # loss function\n",
    "\n",
    "# Model Training function. \n",
    "def epochTraining(model, iterate, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    epochLoss = 0\n",
    "    \n",
    "    for src, trg in iterate:\n",
    "        \n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg) # forward pass.\n",
    "\n",
    "        output = output[:, 1:].contiguous().view(-1, oVocabSize)\n",
    "        trg = trg[:, 1:].contiguous().view(-1)\n",
    "        loss = criterion(output, trg) # calculating loss. \n",
    "        loss.backward() # backward pass. \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epochLoss += loss.item()\n",
    "    \n",
    "    return epochLoss / len(iterate) # average loss for epoch\n",
    "\n",
    "# For evaluating model. \n",
    "def epochEval(model, iterate, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    epochLoss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for src, trg in iterate:\n",
    "            \n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "            output = model(src, trg) # forward pass\n",
    "\n",
    "            output = output[:, 1:].contiguous().view(-1, oVocabSize)\n",
    "            trg = trg[:, 1:].contiguous().view(-1)\n",
    "            loss = criterion(output, trg)\n",
    "            epochLoss += loss.item()\n",
    "    \n",
    "    return epochLoss / len(iterate) # average loss\n",
    "\n",
    "# Training the sequence model for epochs\n",
    "epochCount = 20\n",
    "clip = 1\n",
    "\n",
    "for epoch in range(epochCount):\n",
    "    \n",
    "    tLoss = epochTraining(model, trainLoad, optimizer, criterion, clip) # for one epoch\n",
    "    valLoss = epochEval(model, valLoad, criterion)\n",
    "    print(f\"Epoch: {epoch+1} | Training Loss: {tLoss:.3f} | Validation Loss: {valLoss:.3f}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Evaluation prediction\n",
    "def evalPredictions(model, dataset):\n",
    "    \n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for src, _ in DataLoader(dataset, batch_size=1):\n",
    "    \n",
    "            src = src.to(device)\n",
    "            output = model(src, torch.zeros_like(src).to(device)) # output sequence\n",
    "            tokenPredict = output.argmax(-1).cpu().numpy()[0] # high-score token\n",
    "            predictions.append(eVocab.decode(tokenPredict))\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "testPredictions = evalPredictions(model, testData)\n",
    "\n",
    "with open(\"predictions.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(testPredictions))\n",
    "\n",
    "with open(\"references.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(eTest))\n",
    "\n",
    "# Run the following commands in cmd for Bleu-Results:\n",
    "# perl multi-bleu.perl predictions.txt < references.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
